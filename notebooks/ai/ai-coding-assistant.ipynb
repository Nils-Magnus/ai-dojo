{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Coding Assistants\n",
    "\n",
    "![](../graphics/artwork/copilot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022: AI Coding Assistants are entering the market\n",
    "\n",
    "- [*GitHub Copilot*](https://github.com/features/copilot/)\n",
    "- [*tabnine*](https://www.tabnine.com)\n",
    "- [*codiga*](https://www.codiga.io)\n",
    "- ...\n",
    "\n",
    "What is the technology is behind them? How do they perform? And how will they impact the work of developers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox \n",
    "\n",
    "[**GitHub Copilot Sandbox**](ai-copilot-sandbox.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: Will AI change the way we code (for the better)?\n",
    "\n",
    "_Do you have experience with AI coding assistants? What are your thoughts on the topic? Which advantages and disadvantages do you see?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What powers GitHub Copilot?\n",
    "\n",
    "The machinery behind GitHub Copilot is [**OpenAI Codex**](https://openai.com/blog/openai-codex/), a large language model trained on source code. \n",
    "\n",
    "- OpenAI codex is a descendant of [**GPT-3**](https://en.wikipedia.org/wiki/OpenAI#GPT-3), a large language model trained on natural language text from the web.\n",
    "- The model is trained on 1.5 billion lines of code from GitHub\n",
    "  - ... and **fine tuned** on a smaller, curated set of high quality code\n",
    "- OpenAI Codex has much of the natural language understanding of GPT-3, but it produces working code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Learning Models\n",
    "\n",
    "- Training a _language model_ is a **sequence learning** task\n",
    "  - **sequence learning**: any machine learning task where the input is a sequence of values or tokens\n",
    "\n",
    "- **vector-to-sequence**: \n",
    "  - e.g. _image captioning_: image -> sequence of words\n",
    "  \n",
    "- **sequence-to-vector**: \n",
    "  - e.g. _sentiment analysis_: sequence of words -> sentiment\n",
    "  \n",
    "- **sequence-to-sequence**:\n",
    "  - e.g. _machine translation_: sequence of words in one language -> sequence of words in another language\n",
    "  - e.g. _text summarization_: sequence of words -> sequence of words (shorter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of Sequence Learning\n",
    "\n",
    "RNN -> LSTM -> Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN)\n",
    "\n",
    "Unlike **feedforward neural network**, a **recurrent neural network** can use its internal state to process sequences of inputs. That gives the network _sequential memory_.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Dana-Hughes/publication/305881131/figure/fig5/AS:391681317851147@1470395511494/Feed-forward-and-recurrent-neural-networks.png)\n",
    "\n",
    "pros:\n",
    "- can process sequences of inputs\n",
    "\n",
    "cons:\n",
    "- slow to train (sequential processing of sequence)\n",
    "- bad at learning from long sequences - \"short memory\" \n",
    "  - due to _vanishing gradient problem_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Networks\n",
    "\n",
    "- **Long Short-Term Memory** is a recurrent neural network architecture. \n",
    "- LSTM networks contain **LSTM units**, in which a **memory cell** can store values and **gates** regulate read, write and delete operations on the cell. \n",
    "- During training, the LSTM network can learn how to operate memory in order to remember the information that is relevant for the task. \n",
    "\n",
    "\n",
    "\n",
    "pros:\n",
    "- can learn from _longer_ sequences of inputs\n",
    "  - by solving the _vanishing gradient problem_\n",
    "  \n",
    "cons:\n",
    "- slower to train (sequential processing of sequence, more complex than simple RNN)\n",
    "- not truly bidirectional - but for language context left and right of a word is important\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "- a **transformer** is a **sequence model** which is not a recurrent neural network, instead using an **attention mechanism** \n",
    "\n",
    "pros:\n",
    "- can learn from _even longer_ sequences\n",
    "  - in theory infinite, constrained by compute resources  \n",
    "- fast to train (parallel processing of sequence)\n",
    "- truly bidirectional context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Large Language Models (e.g. GPT)\n",
    "\n",
    "GPT () is a language model. It is based on the **Transformer** architecture. The model was trained on a large corpus of code snippets and can be used to generate code snippets. \n",
    "\n",
    "**What is a language model?**\n",
    "\n",
    "Language models have a large variety of use cases:\n",
    "- **text generation**\n",
    "- **text classification**\n",
    "- **text summarization**\n",
    "- **question answering**\n",
    "- **machine translation**\n",
    "- **speech recognition**\n",
    "\n",
    "\n",
    "A language model has learned a **probability distribution** over sequences of words.\n",
    "\n",
    "\n",
    "![](https://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif)\n",
    "_Source: [The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/)_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/582/1*C-KNWQC_wXh-Q2wc6VPK1g.png)\n",
    "<br>\n",
    "_Source: [GPT-3: The New Mighty Language Model from OpenAI](https://towardsdatascience.com/gpt-3-the-new-mighty-language-model-from-openai-a74ff35346fc)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Codex\n",
    "\n",
    "todo?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Will AI change the way we code (for the better)? - the evidence so far\n",
    "\n",
    "- Does GitHub Copilot lead to higher productivity?\n",
    "  - GitHub's own user study says yes\n",
    "  - independent studies are so far inconclusive\n",
    "- Does GitHub Copilot lead to better code?\n",
    "  - Copilot may reproduce security flaws: [_Do Users Write More Insecure Code with AI Assistants?_](https://arxiv.org/pdf/2211.03622.pdf)\n",
    "   \n",
    "    > \"We observed that participants who had access to the AI assistant were more likely to introduce security vulnerabilities for the majority of programming tasks, yet also more likely to rate their insecure answers as secure compared to those in our control group.\"\n",
    "\n",
    "    > \"Additionally, we found that participants who invested more in the creation of their queries to the AI assistant, such as providing helper functions or adjusting the parameters, were more likely to eventually provide secure solutions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [The Illustrated GPT-2 (Visualizing Transformer Language Models)\n",
    "](https://jalammar.github.io/illustrated-gpt2/)\n",
    "- [Transformer Neural Networks - EXPLAINED! (Attention is all you need)](https://www.youtube.com/watch?v=TQQlZhbC5ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2022 [Christian Staudt](https://clstaudt.me)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "8230f8769c668c567fa0bfc92e1f33442249cb35a608457cf81f184d81994cb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
