{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c11f49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Minimizing models (name TBD)\n",
    "\n",
    "title slide image tbd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c96fff",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effe5b6f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "_notebook configuration_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd92535",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model sizes have been growing massively over the years\n",
    "\n",
    "* plot here showing how number of parameters has grown\n",
    "* maybe also a plot about FLOPS?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845c114",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training such models needs a lot of resources\n",
    "\n",
    "* look up number of GPUs, training time, estimated carbon footprint for one of the recent models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559f06b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Only few research labs have the resources needed for training\n",
    "\n",
    "* That histogram that shows which companies have how many GPUs\n",
    "* maybe something about the open science initiatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba07af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer learning / foundational models\n",
    "\n",
    "* do the expensive training only once\n",
    "* then fit for the specific use case\n",
    "* find some nice graphic here that shows that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032bffb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Do the models need to be so big?\n",
    "\n",
    "* pick up a bit of the scale discussion happening recently, maybe there is a nice plot?\n",
    "* we don't know yet if we can achieve the same with smaller models\n",
    "* currently in research a trend to just throw more computing at the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280af26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advantages of smaller models\n",
    "\n",
    "* less server infrastructure needed (no GPU)\n",
    "* might even run locally on device\n",
    "  * privacy - data does not go into cloud (not very relevant to Telekom cloud people I guess)\n",
    "  * even less server infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d419c28d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* showcase TinyML movement\n",
    "* tensorflow lite / pytorch mobile for on-phone / in browser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93034f51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Making large models smaller: pruning\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../graphics/pruning.png\" width=\"700\">\n",
    "\n",
    "_Image credits: https://arxiv.org/pdf/1506.02626.pdf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a35f7c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Making large models smaller: pruning\n",
    "\n",
    "<img src=\"../graphics/prune+finetune.svg\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029c5e2",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "notes to self about pruning:\n",
    "* usually in a convnet would remove complete filters, but I think the fully connected layer illustration above is easier to understand\n",
    "* alternative to low-weight neurons: gradient-magnitude pruning, but we don't really talk about gradients, do we?\n",
    "* there are more complex versions of the above scheme, but I don't think they are necessary here\n",
    "* there is also something with pruning right from the start, again, let's keep it simple\n",
    "* this medium article has __loads__ of links to papers, I mainly read the first pruning paper from 2015 https://towardsdatascience.com/neural-network-pruning-101-af816aaea61"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8e856",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Making large models smaller: knowledge distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e9bfa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "why does this work:  \"An image of a BMW, for example, may only have\n",
    "a very small chance of being mistaken for a garbage truck, but that mistake is still many times more\n",
    "probable than mistaking it for a carrot\" (https://arxiv.org/pdf/1503.02531.pdf). a large model can pick up on that despite being trained only on one-hot vectors that can not encode these nuances. a small model can not pick up on that when trained on the original data, but by training on the more fuzzy outputs from the large model we can still get that knowledge into the model.\n",
    "\n",
    "todo: make the above into nice pictures :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b212ed",
   "metadata": {},
   "source": [
    "should we do a practical exercise for pruning or knowledge distillation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32187f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
